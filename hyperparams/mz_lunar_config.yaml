PER:
    value: true
PER_alpha:
    value: 0.5
action_space:
    value:
        - 0
        - 1
        - 2
        - 3
batch_size:
    value: 128
blocks:
    value: 3
channels:
    value: 16
checkpoint_interval:
    value: 10
conv_layers_trans:
    value:
        - - 32
          - 1
          - 1
        - - 64
          - 1
          - 1
cum_reward:
    value: false
debug_mode:
    value: false
discount:
    value: 0.999
downsample:
    value: false
encoding_loss_weight:
    value: null
encoding_size:
    value: 10
fc_dynamics_layers:
    value:
        - 64
fc_layers_trans:
    value:
        - 64
fc_policy_layers:
    value:
        - 64
fc_representation_layers:
    value: []
fc_reward_layers:
    value:
        - 64
fc_value_layers:
    value:
        - 64
game_name:
    value: lunarlander_org
gif_name:
    value: test
logger:
    value: wandb
loss_weight_decay:
    value: null
lr_decay_rate:
    value: 1
lr_decay_steps:
    value: 10000
lr_init:
    value: 0.005
max_moves:
    value: 700
max_num_gpus:
    value: null
max_seq_length:
    value: 50
max_time_minutes:
    value: 540
mlp_head_layers:
    value:
        - 16
momentum:
    value: 0.9
muzero_player:
    value: 0
network:
    value: resnet
norm_layer:
    value: true
num_simulations:
    value: 50
num_unroll_steps:
    value: 10
num_workers:
    value: 1
observation_shape:
    value:
        - 1
        - 1
        - 8
opponent:
    value: null
optimizer:
    value: Adam
pb_c_base:
    value: 19652
pb_c_init:
    value: 1.25
players:
    value:
        - 0
policy_network:
    value: transformer
positional_embedding_type:
    value: sinus
predict_reward:
    value: true
project:
    value: TransZeroV3
ratio:
    value: null
reanalyse_on_gpu:
    value: true
reduced_channels_policy:
    value: 4
reduced_channels_reward:
    value: 4
reduced_channels_value:
    value: 4
render_mode:
    value: null
replay_buffer_size:
    value: 2000
representation_network_type:
    value: mlp
resnet_fc_policy_layers:
    value:
        - 16
resnet_fc_reward_layers:
    value:
        - 16
resnet_fc_value_layers:
    value:
        - 16
reward_network:
    value: transformer
root:
    value: /kaggle/working/TransZero
root_dirichlet_alpha:
    value: 0.25
root_exploration_fraction:
    value: 0.25
save_interval:
    value: 10000
save_model:
    value: true
self_play_delay:
    value: 0
selfplay_on_gpu:
    value: true
show_preds:
    value: false
softmax_limits:
    value:
        - 0.2
        - 0.4
        - 0.6
        - 0.8
        - 1
softmax_temps:
    value:
        - 0.45
        - 0.4
        - 0.35
        - 0.2
        - 0.05
stable_transformer:
    value: false
stacked_observations:
    value: 0
state_size:
    value: null
stopping_criterion:
    value: num_played_steps
support_size:
    value: 20
td_steps:
    value: 30
temperature_threshold:
    value: null
testing:
    value: false
train_on_gpu:
    value: true
training_delay:
    value: 0
training_steps:
    value: 300000
transformer_heads:
    value: 16
transformer_hidden_size:
    value: 64
transformer_layers:
    value: 4
use_last_model_value:
    value: true
use_proj:
    value: false
value_loss_weight:
    value: 1
value_network:
    value: transformer
warmup_steps:
    value: 12500
weight_decay:
    value: 0.0001
